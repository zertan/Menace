# PTR Extraction Software
This bundle of scripts and utilities is a basic implementation of the algorithm for extracting Peak-to-Trough Ratios from Metagenomic data, as first described in [(Korem et. al, Science, 2015)](http://science.sciencemag.org/content/349/6252/1101).

## Contents
Below follows a description of the main scripts in the package.

#### jobscript
A submit script for sending a batch job to slurm for parallel processing on a computing cluster.

**input:** none
**output:** none

#### mainBuild.sh
The main build script with commands intended to be executed on the cluster. 

**input:** none
**output:** temporary paths and files on computing nodes

#### PTRMatrix.py
Traverses the specified directory generated by mainBuild.sh and assembles information from each sample into tabular form (eg. averages origin locations from many samples for a better estimate).

**input:** $OUTPUT\_PATH, $DORIC\_PATH, $REF\_PATH, $REF\_ORIC\_PATH
**output:** Abundance.csv, PTR.csv, DoublingTime.csv, Header.csv

#### piecewiseFit.py
Implements the piecewise linear fit and prior checks on the generated depth files to filter out those instances in which enough data was generated to produce a reliable coverage signal for estimating replication origins. This data can be used further on, once those has been estimated using the full cohort, to produce PTR-vaules for each sample.

**input:** 
**output:** 

#### fetchSeq.py
This utility can be used to download '.fasta' reference files from the NCBI servers.

**input:** searchStrings.txt, 
**output:** {reference.fasta}

## Installation:

#### Git
```bash
git clone git@github.com:zertan/PTR-pipeline.git
cd PTR-pipeline
python setup.py install
```

This should install the package including the below *python* dependencies. The other dependencies have to be installed manually (if you have questions about this I suggest you consult your cluster IT help desk).

### Dependencies:

python>=2.7
Biopython
numpy
scipy
pandas
xmltodict
lmfit

[samtools](http://www.htslib.org/download/)
[bamtools](https://github.com/pezmaster31/bamtools/wiki/Building-and-installing)
[bowtie2](https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.2.9/)
[Pathoscope 2.0](https://sourceforge.net/projects/pathoscope/files/?source=navbar)
[parallel](http://www.gnu.org/software/parallel/)

[DoriC](http://tubic.tju.edu.cn/doric/download.php) is a databse of chromosome origin locations (OriCs) which is a (recommended) optional dependency for the pipeline. Please visit the link and enter your e-mail to download.

## Usage
A short version of this usage can be found [below](## Short usage).

#### 1 Follow the above [Installation](##Installation) procedure.

#### 2 Update config.sh and loadmodules.sh
Edit the necassry information in config.sh and do a ´source config.sh´. Most cluster resources load software as modules. Update loadmodules.sh and make sure you have the correct module names for python with numpy and scipy loaded.

#### 3 Download Metagenomic data to your cluster.
Ex. Download sample ERR525688-ERR525787 of cohort ERR525 using four wget processes.

```bash
let e="787";let s="688";let n="($e-$s+1)/5-1";
DATA_PATH="/path/to/store/data"
COHORT_URL="ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR525"
seq $s $e | parallel -j 4 wget -r --no-parent -P $DATA_PATH $COHORT_URL/ERR525{}/
```

#### 3 Download genome references.
Ex. Use 'fetchSeq.py' to download genome references (specified in referenceACC.txt) as '.fasta' files from NCBI. Use "-t" to also download taxonomy information used in later steps.

```bash
./fetchSeq.py -e $EMAIL -t True -s referenceACC.txt -d RefSeq
```

Alternatively download directly from the [ftp site](ftp://ftp.ncbi.nlm.nih.gov/).

#### 4 Build bowtie2 index.
Before building the index, taxonomoy information must be added to the reference files (this is used by Pathoscope for strain specific read redistribution):

```bash
while IFS=$'\t' read -r -a arr
do
	sed -i "" "s/^>gi/>ti|${arr[1]}|gi/" "${REF_PATH}/Fasta/${arr[0]}.fasta"
done < ${REF_PATH}/taxIDs.txt
```

Now build the index:
```bash
mkdir ${REF_PATH}/Index
bowtie2-build --large-index $(ls -1 --color=none ${REF_PATH}/Fasta | tr '\n' ',') ${REF_PATH}/Index/${REF_NAME}
```

#### 6 Submit jobscript to cluster
Please note that sbatch *must* be called from within the PTR-Pipeline directory.

```bash
cd $SCRIPT_PATH
sbatch --array=0-$n jobscript
```
Logs are output to '$SCRIPT_PATH/*.stderr' and '$SCRIPT_PATH/*.stdout'.

#### 7 Collect the output data into dataframes

```bash
./PTRMatrix.py ${DATA_PATH} ${REF_PATH} ${DORIC_PATH}
```

## Short usage
If you are contempt with using the reference file "referenceACC.txt" (a set of bacterial genome accession numbers commonly present in the human gastrointestinal tract) all you need to do is set the variables in `config.sh` and then run `./ptrBuild.sh`. This will download references and build an index, and submit the job to the slurm queue. Then perform step 7 above to collect the data.

## Directory structure (*recommended example*)
With the above usage example the path structure(s) will look something like below.

```
$DATA_PATH
  ├ "Sample01"								(eg. ERR525688)
  .  ├ {sample01_1.fastq.gz}
  .  └ {sample01_2.fastq.gz} 				paired metagenomic reads
  .

$REF_PATH
  ├ Index
  |  └ {$REF_NAME.*.bt2l}					bowtie2 index files
  └ Headers
     └ {accession.xml}						xml files containing extra genome references info

$DORIC_PATH
  ├ bacteria_record.dat
  └ bacteria_seq.fas

$OUTPUT_PATH
  ├ "Sample01"
  .  ├ depth
  .  |  └ {accession.depth} 				coverage files for each reference
  .  ├ log
     |  └ {accession.log}					output logs from piecewiseFit	
     ├ npy
     |  └ {accession_OriC_TerC.npy}			numpy files with origin/terminus locations
     ├ png
     |  └ {accession_fit.png}  				images of piecewise fit of the smoothed coverage
     └ accession-sam-report.tsv				Pathoscope2 reassignment report
```
